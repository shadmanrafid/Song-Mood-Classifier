{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main-French.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPaa5OkqI+4D6JftRSHmXzc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8RePj21GXCbw","colab_type":"text"},"source":["# **PREPROCESSING**"]},{"cell_type":"markdown","metadata":{"id":"900iyTzlW0ph","colab_type":"text"},"source":["Reading Training DF"]},{"cell_type":"code","metadata":{"id":"ZmxQnH9kR8hy","colab_type":"code","outputId":"9516cb1c-5fed-4ca0-cb55-e41bb0cca651","executionInfo":{"status":"ok","timestamp":1588199765940,"user_tz":420,"elapsed":393,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Training data of 400 songs\n","import pandas as pd\n","\n","df = pd.read_csv('/content/french_training_data.csv')\n","df.columns = ['Lyrics', 'Mood']\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lyrics</th>\n","      <th>Mood</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Une histoire qui tombe √† l'eau\\nQuelques mots...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Cas√© dans des cases, fich√© sur des fiches\\nA...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ohh...\\n\\nChanter, chanter pour se dessiner un...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Le pardon commence o√π s'arr√™te la rancune\\nI...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(Kery James)\\nTu peux pas t'sais, c'est profon...</td>\n","      <td>Happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Lyrics   Mood\n","0  Une histoire qui tombe √† l'eau\\nQuelques mots...    Sad\n","1  Cas√© dans des cases, fich√© sur des fiches\\nA...    Sad\n","2  Ohh...\\n\\nChanter, chanter pour se dessiner un...    Sad\n","3  Le pardon commence o√π s'arr√™te la rancune\\nI...    Sad\n","4  (Kery James)\\nTu peux pas t'sais, c'est profon...  Happy"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"ejgUTbNuWzgk","colab_type":"text"},"source":["Encoding Mood Values"]},{"cell_type":"code","metadata":{"id":"pfooUdM4XIi1","colab_type":"code","outputId":"479ba536-e914-48b4-871d-1738ae49e070","executionInfo":{"status":"ok","timestamp":1588199768733,"user_tz":420,"elapsed":618,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.preprocessing import LabelEncoder\n","# import pickle\n","import numpy as np\n","\n","x_train = df['Lyrics'].values \n","y_train = df['Mood'].values\n","\n","print('before: %s ...' %y_train[:20])\n","\n","le = LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","\n","print('after: %s ...' %y_train[:20])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["before: ['Sad' 'Sad' 'Sad' 'Sad' 'Happy' 'Happy' 'Sad' 'Sad' 'Sad' 'Sad' 'Happy'\n"," 'Sad' 'Sad' 'Sad' 'Happy' 'Happy' 'Happy' 'Sad' 'Happy' 'Sad'] ...\n","after: [1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1] ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-nlDpicFZr98","colab_type":"code","outputId":"b75db8f9-2e1e-486f-dd85-502a0e57e2b5","executionInfo":{"status":"ok","timestamp":1588199774308,"user_tz":420,"elapsed":3695,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40IT85OrZuh7","colab_type":"text"},"source":["Stemming (SnowballStemmer)"]},{"cell_type":"code","metadata":{"id":"0SxSGMKvaNrX","colab_type":"code","outputId":"7ed1b312-a872-43ca-cf92-655b291bd3cc","executionInfo":{"status":"ok","timestamp":1588199808167,"user_tz":420,"elapsed":920,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\" The following languages are supported: Danish, Dutch, English, Finnish, French, \n","German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish \n","and Swedish \"\"\"\n","import nltk\n","import re\n","from nltk import word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","\n","def snowball_tokenizer(text, stemmer = SnowballStemmer('french')):\n","  lower_txt = text.lower()\n","  tokens = nltk.wordpunct_tokenize(lower_txt)\n","  stemmed_text = [stemmer.stem(i) for i in tokens]\n","  no_punct = [s for s in stemmed_text if re.match('^[a-zA-Z]+$', s) is not None]\n","  return stemmed_text\n","\n","\n","snowball_tokenizer(\"Aller\")\n","\n","# stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['aller']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"GZfadnzarHo0","colab_type":"text"},"source":["Stopwords File"]},{"cell_type":"code","metadata":{"id":"_BlWp7LlasiQ","colab_type":"code","outputId":"b4613ca2-d644-46d4-bfb2-32dfce51145b","executionInfo":{"status":"ok","timestamp":1588199811542,"user_tz":420,"elapsed":511,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stp = stopwords.words('french')\n","with open('./stopwords_french.txt', 'w') as outfile:\n","   outfile.write('\\n'.join(stp))\n","\n","with open('./stopwords_french.txt', 'r') as infile:\n","    stop_words = infile.read().splitlines()\n","print('stop words %s ...' %stop_words[:5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","stop words ['au', 'aux', 'avec', 'ce', 'ces'] ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BcS7Z6ABa-vD","colab_type":"text"},"source":["# **VECTORIZING METHODS**"]},{"cell_type":"markdown","metadata":{"id":"y3z5Y2hbbCe9","colab_type":"text"},"source":["## Feature extraction: Word counts and Vectorizers"]},{"cell_type":"markdown","metadata":{"id":"I8d5ACwXbLt4","colab_type":"text"},"source":["Count Vectorizer"]},{"cell_type":"code","metadata":{"id":"rF7YvHgbbN7A","colab_type":"code","outputId":"0e82fb1e-47a1-4bf3-9bc0-48afc9b671ec","executionInfo":{"status":"ok","timestamp":1588199817965,"user_tz":420,"elapsed":369,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer,\n","            ngram_range=(1,1)\n","    )\n","\n","vocab = [\"Je vais à la plage. À plus tard!\"]\n","\n","vec = vec.fit(vocab)\n","\n","sentence1 = vec.transform([u'Plus tard sur la plage!'])\n","sentence2 = vec.transform(['Je dois y aller'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %vec.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['!', '.', 'a', 'plag', 'plus', 'tard', 'vais']\n","Sentence 1: [[1 0 0 1 1 1 0]]\n","Sentence 2: [[0 0 0 0 0 0 0]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'etaient', 'etais', 'etait', 'etant', 'ete', 'eti', 'etion', 'eum', 'euss', 'eussion', 'fum', 'fuss', 'fussion', 'mem', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"slnzVCpocs5L","colab_type":"code","outputId":"a564c3f6-a0d3-4184-d6b1-7a82c1d83aeb","executionInfo":{"status":"ok","timestamp":1588199844728,"user_tz":420,"elapsed":15094,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["vec = vec.fit(x_train.ravel())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"k7-iZl3dc4UU","colab_type":"text"},"source":["TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"HeCFsnNvc7Uq","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8G9Slr7dAAl","colab_type":"code","outputId":"d79b06a8-ae02-4fa0-ee0c-26c501575a52","executionInfo":{"status":"ok","timestamp":1588199863625,"user_tz":420,"elapsed":374,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["vocab = [\"Je vais à la plage. À plus tard!\"]\n","\n","tfidf = tfidf.fit(vocab)\n","\n","sentence1 = tfidf.transform([u'Plus tard sur la plage!'])\n","sentence2 = tfidf.transform(['Je dois y aller \\n'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %tfidf.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['!', '.', 'a', 'plag', 'plus', 'tard', 'vais']\n","Sentence 1: [[0.5 0.  0.  0.5 0.5 0.5 0. ]]\n","Sentence 2: [[0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'etaient', 'etais', 'etait', 'etant', 'ete', 'eti', 'etion', 'eum', 'euss', 'eussion', 'fum', 'fuss', 'fussion', 'mem', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-sGlqCUUdVyq","colab_type":"code","outputId":"97eba8ee-b41c-4bf5-f899-33b79ac9b9a7","executionInfo":{"status":"ok","timestamp":1588199881061,"user_tz":420,"elapsed":15236,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["tfidf = tfidf.fit(x_train.ravel())\n","\n","print('Vocabulary size: %s' %len(tfidf.get_feature_names()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"},{"output_type":"stream","text":["Vocabulary size: 16286\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rDBvr5GWdbCX","colab_type":"text"},"source":["# **Model Selection**"]},{"cell_type":"markdown","metadata":{"id":"DkfCRuMddd85","colab_type":"text"},"source":["### **Models and F1-Score**"]},{"cell_type":"code","metadata":{"id":"N_QKREUkdahN","colab_type":"code","colab":{}},"source":["#Models: Multivariate Bernoulli and Multinomial Naive Bayes\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.pipeline import Pipeline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGcP2X8wdlpv","colab_type":"code","colab":{}},"source":["# Performance metric: F1-score\n","\n","# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mpDDQ1Ldoja","colab_type":"text"},"source":["## **Grid Search**"]},{"cell_type":"code","metadata":{"id":"MLSic8_mdmvJ","colab_type":"code","outputId":"4b0767ae-5fd2-4a82-d658-081d314c953a","executionInfo":{"status":"ok","timestamp":1588199892580,"user_tz":420,"elapsed":2830,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install scikit-learn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DL98HM7WdwPa","colab_type":"code","outputId":"fa5d5711-78af-4553-d129-21f37792901d","executionInfo":{"status":"error","timestamp":1587897692430,"user_tz":420,"elapsed":16962,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":582}},"source":["# Grid Search with Count Vectorizer and Bernoulli Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","\n","pipeline_1 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_1 = dict(\n","    vect__binary=[True],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_1 = GridSearchCV(pipeline_1, \n","                           parameters_1, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10                #Determines the cross-validation splitting strategy\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n","print(\"parameters:\")\n","pprint(parameters_1, depth=2)\n","grid_search_1.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_1 = grid_search_1.best_estimator_.get_params()\n","for param_name in sorted(parameters_1.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [True],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7f3167111d90>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b502ccde7979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mgrid_search_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_search_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[0;32m---> 87\u001b[0;31m                                       *args, **kwargs)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 212\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1315\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n","\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."]}]},{"cell_type":"code","metadata":{"id":"7LkJ0wAegfiW","colab_type":"code","outputId":"1d51cecb-a881-4c25-f7c2-49d0c7712666","executionInfo":{"status":"ok","timestamp":1587253871880,"user_tz":420,"elapsed":154941,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with Count Vectorizer and Multinomial Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_3 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_3 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_3 = GridSearchCV(pipeline_3, \n","                           parameters_3, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n","print(\"parameters:\")\n","pprint(parameters_3, depth=2)\n","grid_search_3.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_3 = grid_search_3.best_estimator_.get_params()\n","for param_name in sorted(parameters_3.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7f59d7ea9730>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.5min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.480\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7f59d7ea9730>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfM8Oupoh020","colab_type":"code","outputId":"142d20d6-d100-463a-fa5d-d9917e2983d8","executionInfo":{"status":"ok","timestamp":1587254032999,"user_tz":420,"elapsed":156905,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Bernoulli Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_2 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_2 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_2 = GridSearchCV(pipeline_2, \n","                           parameters_2, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_2.steps])\n","print(\"parameters:\")\n","pprint(parameters_2, depth=2)\n","grid_search_2.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_2.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_2 = grid_search_2.best_estimator_.get_params()\n","for param_name in sorted(parameters_2.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_2[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7f59d7ea9730>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.5min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.177\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7f59d7ea9730>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WWWYGy8gjLwz","colab_type":"code","outputId":"0422569b-75c3-44d8-eadf-425fe219546c","executionInfo":{"status":"ok","timestamp":1587254224181,"user_tz":420,"elapsed":151836,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Multinomial Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_4 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_4 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_4 = GridSearchCV(pipeline_4, \n","                           parameters_4, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n","print(\"parameters:\")\n","pprint(parameters_4, depth=2)\n","grid_search_4.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_4 = grid_search_4.best_estimator_.get_params()\n","for param_name in sorted(parameters_4.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7f59d7ea9730>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.035\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (3, 3)\n","\tvect__stop_words: ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n","\tvect__tokenizer: None\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.5min finished\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-oCoaBr2k0wM","colab_type":"text"},"source":["# **VALIDATION**"]},{"cell_type":"code","metadata":{"id":"ewcbLj0dk13w","colab_type":"code","outputId":"9381b594-ed37-4ef2-e231-9f2c736cf080","executionInfo":{"status":"ok","timestamp":1588199998543,"user_tz":420,"elapsed":15069,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["# Grid SEarch Best Score = 0.\n","final_clf = Pipeline([\n","                ('vect', CountVectorizer(\n","                                         binary=False,\n","                                         stop_words=stop_words,\n","                                         tokenizer=snowball_tokenizer,\n","                                         ngram_range=(1,1),\n","                                         )\n","                ),\n","                ('clf', MultinomialNB(alpha=1.0)),\n","               ])\n","final_clf.fit(x_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm', 'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=['au', 'aux', 'avec', 'ce', 'ces',\n","                                             'dans', 'de', 'des', 'du', 'elle',\n","                                             'en', 'et', 'eux', 'il', 'ils',\n","                                             'je', 'la', 'le', 'les', 'leur',\n","                                             'lui', 'ma', 'mais', 'me', 'même',\n","                                             'mes', 'moi', 'mon', 'ne', 'nos', ...],\n","                                 strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=<function snowball_tokenizer at 0x7fe0715ec510>,\n","                                 vocabulary=None)),\n","                ('clf',\n","                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"ryXSSGIClFdy","colab_type":"text"},"source":["## TESTING ON TRAINING DATA"]},{"cell_type":"code","metadata":{"id":"_gmCdbEblIVv","colab_type":"code","outputId":"792deca0-5d8f-4af1-d013-b7a28044d015","executionInfo":{"status":"ok","timestamp":1588200018949,"user_tz":420,"elapsed":15188,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import matplotlib as mpl\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas.util.testing as tm\n","\n","cm = metrics.confusion_matrix(y_train, final_clf.predict(x_train))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title('French Confusion Matrix - Training dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('French_Training.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ETWzN3pBlUgl","colab_type":"text"},"source":["## TESTING ON VALIDATION DATA"]},{"cell_type":"code","metadata":{"id":"hPj1gc0ulTMD","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","import pickle\n","import numpy as np\n","\n","df = pd.read_csv('/content/french_testing_data.csv')\n","df.columns = ['Lyrics', 'Mood']\n","df.head()\n","\n","X_valid = df['Lyrics'].values \n","y_valid = df['Mood'].values\n","\n","\n","# le = LabelEncoder()\n","# le.fit(y_train)\n","y_valid = le.transform(y_valid)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-z7Qa_ZJY8IH","colab_type":"code","outputId":"bf0e98d9-4e7a-4a29-f082-f495fb08b930","executionInfo":{"status":"ok","timestamp":1588200042086,"user_tz":420,"elapsed":352,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lyrics</th>\n","      <th>Mood</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Si ce n√©tait que mon √©preuve\\nPour avoir dro...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tu sais d'o√π je viens \\nEt par quoi je suis p...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>J'ai obli√© qui j'attend depuis quand, comment...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Andr√©eWatters/Pat Frazer)\\n\\nTu pars, les ye...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Urgence\\nPartout dans le monde\\nLe diable dans...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>395</th>\n","      <td>(Andr√©e Watters/Catherine Durand)\\n\\nJe vis d...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>396</th>\n","      <td>Minuit le soir, le monde s'endort \\nLa nuit s'...</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>397</th>\n","      <td>J'ai donn√© chance √† mon coeur\\nD'√©garer les...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>398</th>\n","      <td>(Andr√©e Watters/Styve Bolduc)\\n\\nIl y a des r...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>399</th>\n","      <td>(Andr√©e Watters, Fred St-Gelais)\\n\\nTous mes ...</td>\n","      <td>Sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                Lyrics   Mood\n","0    Si ce n√©tait que mon √©preuve\\nPour avoir dro...    Sad\n","1    Tu sais d'o√π je viens \\nEt par quoi je suis p...    Sad\n","2    J'ai obli√© qui j'attend depuis quand, comment...    Sad\n","3    (Andr√©eWatters/Pat Frazer)\\n\\nTu pars, les ye...    Sad\n","4    Urgence\\nPartout dans le monde\\nLe diable dans...    Sad\n","..                                                 ...    ...\n","395  (Andr√©e Watters/Catherine Durand)\\n\\nJe vis d...    Sad\n","396  Minuit le soir, le monde s'endort \\nLa nuit s'...  Happy\n","397  J'ai donn√© chance √† mon coeur\\nD'√©garer les...    Sad\n","398  (Andr√©e Watters/Styve Bolduc)\\n\\nIl y a des r...    Sad\n","399  (Andr√©e Watters, Fred St-Gelais)\\n\\nTous mes ...    Sad\n","\n","[400 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"HmNFPJlalmWN","colab_type":"code","outputId":"56b23215-cd3b-468d-d9e4-faa09d0150f3","executionInfo":{"status":"ok","timestamp":1588200050515,"user_tz":420,"elapsed":2736,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["cm = metrics.confusion_matrix(y_valid, final_clf.predict(X_valid))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title('French Confusion Matrix - Validation dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('French_Testing.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ifTywmdllsUi","colab_type":"text"},"source":["## **METRICS**"]},{"cell_type":"code","metadata":{"id":"zCPavRSclrOT","colab_type":"code","colab":{}},"source":["# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n","pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n","rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKbzOR4-lzlN","colab_type":"code","colab":{}},"source":["d = {'Data':['Training', 'Validation'],\n","     'ACC (%)':[],\n","     'PRE (%)':[],\n","     'REC (%)':[],\n","     'F1 (%)':[],\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFz0LqERl1lb","colab_type":"code","colab":{}},"source":["d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","\n","d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dC76CaQDl7cC","colab_type":"code","outputId":"da20a7c3-0aba-4ef8-d886-795e500c2a33","executionInfo":{"status":"ok","timestamp":1588200132869,"user_tz":420,"elapsed":392,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_perform = pd.DataFrame(d)\n","df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)']]\n","df_perform.index=(['Training', 'Validation'])\n","df_perform = df_perform*100\n","df_perform = np.round(df_perform, decimals=2)\n","df_perform"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ACC (%)</th>\n","      <th>PRE (%)</th>\n","      <th>REC (%)</th>\n","      <th>F1 (%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training</th>\n","      <td>92.05</td>\n","      <td>97.57</td>\n","      <td>82.79</td>\n","      <td>89.57</td>\n","    </tr>\n","    <tr>\n","      <th>Validation</th>\n","      <td>69.25</td>\n","      <td>65.22</td>\n","      <td>39.74</td>\n","      <td>49.38</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            ACC (%)  PRE (%)  REC (%)  F1 (%)\n","Training      92.05    97.57    82.79   89.57\n","Validation    69.25    65.22    39.74   49.38"]},"metadata":{"tags":[]},"execution_count":23}]}]}