{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Refractored main.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GutwAaPGnz8w",
    "colab_type": "text"
   },
   "source": [
    "# **PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcKYRg3oElos",
    "colab_type": "text"
   },
   "source": [
    "Reading Training DF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHqQ7RBw-XvG",
    "colab_type": "code",
    "outputId": "25aa457e-3504-4a28-e6d4-96eb6d8a4ddb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123674540,
     "user_tz": 420,
     "elapsed": 567,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pprint as pp\n",
    "DataFrame = pd.read_table('Spanish_train_data.txt', delimiter=\"*\", names=('Link', 'Lyrics', 'Mood'))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pp.pprint(DataFrame.head())"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "                                                Link  \\\n",
      "0   https://lyrics.fandom.com//wiki/ABBA:Chiquiti...   \n",
      "1   https://lyrics.fandom.com//wiki/ABBA:Conoci%C...   \n",
      "2   https://lyrics.fandom.com//wiki/ABBA:Dame!_Da...   \n",
      "3   https://lyrics.fandom.com//wiki/ABBA:Estoy_So...   \n",
      "4      https://lyrics.fandom.com//wiki/ABBA:Felic...   \n",
      "\n",
      "                                              Lyrics   Mood  \n",
      "0  Chiquitita, dime por qué\\nTu dolor hoy te enca...  Happy  \n",
      "1  Ya no hay más sonrisa\\nTodo finaliza\\n\\nNuestr...    Sad  \n",
      "2  El reloj\\nYa marcó medianoche\\nY otra vez enco...  Happy  \n",
      "3  Yo lo soñé\\nY el corazón\\nMe hablo de amor\\nCo...  Happy  \n",
      "4  No más champagne\\nLa bengala se apagó\\nSolo tú...  Happy  \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw6GiDS6FahD",
    "colab_type": "text"
   },
   "source": [
    "Encoding Mood Values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XN6iyjM_4_z2",
    "colab_type": "code",
    "outputId": "fbe8465d-33af-493b-9601-b2ddbedc9acd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123740587,
     "user_tz": 420,
     "elapsed": 769,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import pickle\n",
    "import numpy as np\n",
    "\n",
    "x_train = DataFrame['Lyrics'].values \n",
    "y_train = DataFrame['Mood'].values\n",
    "\n",
    "print('before: %s ...' %y_train[:5])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "\n",
    "print('after: %s ...' %y_train[:5])"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "before: ['Happy' 'Sad' 'Happy' 'Happy' 'Happy'] ...\n",
      "after: [0 1 0 0 0] ...\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GB6x4oiFlqt",
    "colab_type": "text"
   },
   "source": [
    "Writing to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T2-JM-0a5dbG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_out = open('./lyrics_label_encoder_train_data1.pkl', 'wb')\n",
    "# pickle.dump(le, pickle_out)\n",
    "# pickle_out.close()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aH8dZm14RGpa",
    "colab_type": "code",
    "outputId": "87a0afb6-6e8e-4260-d33e-e1c07d721e1a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071866450,
     "user_tz": 420,
     "elapsed": 4038,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "!pip install nltk"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGv6AmnGFxuX",
    "colab_type": "text"
   },
   "source": [
    "Stemming (SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KJSkoDB36s8L",
    "colab_type": "code",
    "outputId": "25ca4531-b95c-4e32-a11b-f01ec8e3d6d9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123834861,
     "user_tz": 420,
     "elapsed": 973,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "# stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "def snowball_tokenizer(text, stemmer = SnowballStemmer('spanish')):\n",
    "  lower_txt = text.lower()\n",
    "  tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "  stemmed_text = [stemmer.stem(i) for i in tokens]\n",
    "  no_punct = [s for s in stemmed_text if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "  return stemmed_text\n",
    "\n",
    "\n",
    "snowball_tokenizer(\"A través de un libro\")\n",
    "\n",
    "# stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a', 'traves', 'de', 'un', 'libr']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHdBzUIHGBMz",
    "colab_type": "text"
   },
   "source": [
    "Stopwords File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FOXPOKQs8uQw",
    "colab_type": "code",
    "outputId": "da2b14af-bc89-40c9-ae44-45b1afc1fcb5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123837672,
     "user_tz": 420,
     "elapsed": 446,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    }
   },
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stp = stopwords.words('spanish')\n",
    "# with open('./stopwords_spanish.txt', 'w') as outfile:\n",
    "#    outfile.write('\\n'.join(stp))\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stp = stopwords.words('spanish')\n",
    "with open('./stopwords_spanish.txt', 'w') as outfile:\n",
    "   outfile.write('\\n'.join(stp))\n",
    "\n",
    "with open('./stopwords_spanish.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "stop words ['de', 'la', 'que', 'el', 'en'] ...\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hduH-1LrOLP9",
    "colab_type": "text"
   },
   "source": [
    "Count Vectorizer (For 1-gram)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w0KUG-oI-TZw",
    "colab_type": "code",
    "outputId": "8a690468-afb4-4c24-b50d-94ffd0f0cda1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123843503,
     "user_tz": 420,
     "elapsed": 444,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "model_vector = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=snowball_tokenizer,\n",
    "            ngram_range=(1,1)\n",
    "    )\n",
    "\n",
    "vocab = [\"Un lujo más \\nsigo herido\\na pesar\"]\n",
    "\n",
    "model_vector = model_vector.fit(vocab)\n",
    "\n",
    "sentence1 = model_vector.transform([u'Un lujo más \\nsigo herido\\na pesar'])\n",
    "sentence2 = model_vector.transform(['Un lujo más \\nsigo herido\\na pesar'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %model_vector.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: ['her', 'luj', 'mas', 'pes', 'sig']\n",
      "Sentence 1: [[1 1 1 1 1]]\n",
      "Sentence 2: [[1 1 1 1 1]]\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KWn3_wXa_6bo",
    "colab_type": "code",
    "outputId": "6d82145c-6dd3-4f82-9cb7-a1b9678b7bb1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123847665,
     "user_tz": 420,
     "elapsed": 396,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "model_vector = model_vector.fit(x_train.ravel())"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6n0bzaYQACAT",
    "colab_type": "code",
    "outputId": "62a2376b-e5c4-48a7-d88a-baf483a23a33",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123859769,
     "user_tz": 420,
     "elapsed": 382,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "print('Vocabulary size: %s' %len(model_vector.get_feature_names()))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Vocabulary size: 648\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ej2uZawYAGfd",
    "colab_type": "code",
    "outputId": "4613f43b-5b10-4bda-a0a8-5e997662dfb3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123865399,
     "user_tz": 420,
     "elapsed": 359,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "print(model_vector.get_feature_names())"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['!', '!\\\\', \"'\", '(', '(¿', ')\\\\', ',', ',...', ',\\\\', ',\\\\...', '...', '...\\\\', '.\\\\', '.\\\\...', ';\\\\', '?,', '?...\\\\', '?\\\\', '\\\\', 'abi', 'abiert', 'abism', 'abra', 'abrazart', 'abril', 'aburr', 'aca', 'acord', 'adentr', 'advertent', 'adverti', 'afan', 'ag', 'agarr', 'agu', 'ahi', 'ahor', 'air', 'alba', 'alej', 'alert', 'algui', 'algun', 'alla', 'alli', 'alma', 'almas', 'amag', 'amanec', 'amarg', 'amargur', 'ame', 'amo', 'amor', 'anda', 'andan', 'andar', 'ang', 'anil', 'anos', 'apag', 'apuest', 'aqui', 'arde', 'aren', 'arm', 'arrastr', 'arrepient', 'arrodill', 'asi', 'asom', 'asombr', 'asumi', 'atras', 'aun', 'aunqu', 'auror', 'avec', 'ay', 'ayer', 'azul', 'bail', 'be', 'bell', 'bendicion', 'bengal', 'bert', 'bes', 'besart', 'bien', 'blanc', 'boc', 'bord', 'braz', 'brill', 'brind', 'brujul', 'buen', 'busc', 'c', 'cabez', 'cactus', 'cad', 'caer', 'call', 'calm', 'cam', 'camin', 'can', 'cancion', 'cans', 'cant', 'car', 'card', 'carici', 'carin', 'carit', 'carn', 'cas', 'castig', 'cay', 'caz', 'cerc', 'chacarer', 'champagn', 'che', 'chil', 'chiquitit', 'churrit', 'ci', 'ciel', 'cit', 'ciud', 'cl', 'clarin', 'cobr', 'color', 'com', 'comienz', 'comp', 'companer', 'conozc', 'consum', 'content', 'cop', 'copl', 'corazon', 'cos', 'cosit', 'costumbr', 'cotidian', 'cre', 'crei', 'cruz', 'cuand', 'cuant', 'cuec', 'cuent', 'cur', 'd', 'dart', 'debaj', 'deberi', 'dec', 'decis', 'dej', 'dejenl', 'desarm', 'desd', 'dese', 'desech', 'desiert', 'despaci', 'despert', 'despiert', 'destroz', 'det', 'deterior', 'di', 'dia', 'dias', 'dich', 'diez', 'dificil', 'dig', 'dij', 'dim', 'dios', 'dobl', 'dolor', 'dominguer', 'dond', 'dorm', 'dos', 'dud', 'dulc', 'eco', 'enca', 'enco', 'encuentr', 'engan', 'ente', 'entra', 'env', 'error', 'escaler', 'escuch', 'escuel', 'espej', 'esper', 'esquin', 'estam', 'estand', 'estes', 'estrell', 'et', 'etern', 'exist', 'expres', 'facult', 'fald', 'fantasm', 'feliz', 'fern', 'finaliz', 'flor', 'fond', 'fra', 'fracas', 'frent', 'fri', 'fuer', 'fuerz', 'funyi', 'gauch', 'gir', 'got', 'gris', 'grit', 'guerr', 'guitarr', 'gust', 'hab', 'habl', 'habr', 'hac', 'hast', 'hech', 'her', 'hic', 'hij', 'hil', 'himn', 'hol', 'hombr', 'honor', 'hor', 'hostil', 'hoy', 'hub', 'hum', 'i', 'ig', 'import', 'inciert', 'inevit', 'ingrat', 'inmacul', 'inmens', 'intim', 'ire', 'jar', 'jue', 'l', 'labi', 'lagrim', 'lastim', 'lej', 'lejan', 'lengu', 'leo', 'ler', 'levant', 'ley', 'libert', 'llam', 'lleg', 'llen', 'llevat', 'llor', 'lluvi', 'locur', 'lu', 'luc', 'lug', 'lun', 'luz', 'm', 'ma', 'mader', 'madr', 'mal', 'man', 'manan', 'mand', 'maner', 'mar', 'marc', 'march', 'mas', 'mat', 'mazamorr', 'medi', 'medianoch', 'ment', 'menud', 'merec', 'mezcl', 'mie', 'miel', 'mient', 'minut', 'mio', 'mir', 'miradan', 'mism', 'mod', 'mont', 'mostrador', 'motiv', 'moz', 'much', 'muchach', 'muert', 'mund', 'murg', 'n', 'na', 'nac', 'nad', 'nadi', 'namarg', 'naquel', 'nau', 'naun', 'navid', 'nbalanc', 'nbi', 'nbrill', 'nbusc', 'nc', 'ncad', 'ncaer', 'ncaj', 'nco', 'ncom', 'ncon', 'ncontest', 'ncorr', 'ncort', 'ncruz', 'ncu', 'nd', 'nde', 'ndeb', 'ndejam', 'ndici', 'ndistint', 'ndon', 'ndond', 'ndurm', 'ne', 'negr', 'nel', 'nell', 'nen', 'nentr', 'ner', 'nes', 'nest', 'nfrent', 'nhistoriet', 'nin', 'nla', 'nlas', 'nle', 'nlleg', 'nllev', 'nlo', 'nluch', 'nmas', 'nme', 'nmi', 'nmientr', 'nmil', 'nmir', 'nnac', 'nni', 'nno', 'nnos', 'nnuestr', 'nnunc', 'nobl', 'noch', 'nom', 'nort', 'npar', 'nparec', 'nper', 'nperdon', 'nperdonam', 'npi', 'nplan', 'npoc', 'npor', 'nporqu', 'nprend', 'nqu', 'nqui', 'nquis', 'nrefugi', 'nrescold', 'nromp', 'nsab', 'nse', 'nsi', 'nsilenci', 'nsobr', 'nsol', 'nsolucion', 'nsom', 'nson', 'nsoy', 'ntal', 'nteng', 'ntien', 'ntod', 'ntu', 'ntus', 'nub', 'nud', 'nuev', 'nun', 'nunc', 'nviv', 'nvolv', 'nvoy', 'ny', 'nya', 'nyo', 'odi', 'ofrezc', 'oid', 'oir', 'ojit', 'ojo', 'ojos', 'olv', 'olvid', 'ongoy', 'organit', 'p', 'pa', 'palabr', 'palm', 'palpit', 'pan', 'par', 'parec', 'part', 'pas', 'pe', 'pedaz', 'pen', 'pens', 'per', 'perd', 'perdist', 'pers', 'pid', 'piel', 'piens', 'pilch', 'pill', 'pint', 'po', 'pobr', 'poc', 'podr', 'podri', 'poll', 'pon', 'porqu', 'prend', 'pretend', 'prisioner', 'pud', 'puebl', 'pued', 'puent', 'puert', 'pun', 'pus', 'q', 'qu', 'qued', 'quedar', 'quem', 'quer', 'querert', 'qui', 'quier', 'quis', 'razon', 'reci', 'recit', 'recuerd', 'reir', 'reloj', 'riesg', 'rio', 'rob', 'roj', 'romp', 'rond', 'rueg', 's', 'sab', 'sabr', 'sal', 'salg', 'saliv', 'sec', 'segu', 'senal', 'sencill', 'sent', 'sep', 'ser', 'si', 'sie', 'siempr', 'sient', 'sig', 'silenci', 'simpl', 'simplement', 'sirv', 'situ', 'situacion', 'sobr', 'sol', 'soled', 'solucion', 'som', 'sombr', 'sonris', 'sorprend', 'sorprendi', 'suaviz', 'suel', 'suen', 'sur', 't', 'ta', 'taki', 'tal', 'tambor', 'tampoc', 'tan', 'tang', 'tant', 'tard', 'telefon', 'templ', 'tempran', 'tendr', 'teng', 'termi', 'tiembl', 'tiemp', 'tien', 'tierr', 'tin', 'tint', 'tir', 'tira', 'tod', 'todavi', 'tom', 'ton', 'torment', 'trabaj', 'trag', 'traj', 'tras', 'trat', 'trig', 'tropez', 'tucu', 'tucuman', 'unas', 'uni', 'unic', 'usted', 'v', 'va', 'vaci', 'vas', 'vay', 'vec', 'vel', 'vem', 'veng', 'veni', 'veo', 'ver', 'veran', 'ves', 'vesti', 'vez', 'via', 'viaj', 'vibr', 'vid', 'viej', 'vient', 'vigili', 'vin', 'virg', 'vist', 'viv', 'vivi', 'volv', 'vos', 'voy', 'voz', 'vuelt', 'vuelv', 'whisky', 'yem', 'zamb', 'zonz', '¡', '¿', '¿...', '“', '”,\\\\']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m8MgTCXOc4N",
    "colab_type": "text"
   },
   "source": [
    "Count Vectorizer (For 2-gram)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FCdBowyRCUam",
    "colab_type": "code",
    "outputId": "7856d73b-49d0-45bd-d6a4-3e7531352818",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123883364,
     "user_tz": 420,
     "elapsed": 468,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    }
   },
   "source": [
    "model_vector = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=snowball_tokenizer,\n",
    "            ngram_range=(2,2)\n",
    "    )\n",
    "\n",
    "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
    "\n",
    "model_vector = model_vector.fit(vocab)\n",
    "\n",
    "sentence1 = model_vector.transform([u'The swimmer likes swimming.'])\n",
    "sentence2 = model_vector.transform(['The\\nswimmer \\nswims.'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %model_vector.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: [\"' t\", '. don', '1 the', '123 1', '` t', 'didn `', \"don '\", 'lik swimming', 'so swims', 'swimm lik', 'swimming so', 'swims .', 't didn', 'the swimm']\n",
      "Sentence 1: [[0 0 0 0 0 0 0 1 0 1 0 0 0 1]]\n",
      "Sentence 2: [[0 0 0 0 0 0 0 0 0 0 0 1 0 1]]\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TrB6W5TOkz3",
    "colab_type": "text"
   },
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UDt3JmRVCk4K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=snowball_tokenizer\n",
    "    )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E142GHG1CmFM",
    "colab_type": "code",
    "outputId": "90e28336-3cf7-4351-e98c-5d13f7b3fb62",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123972248,
     "user_tz": 420,
     "elapsed": 505,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    }
   },
   "source": [
    "vocab = [\"En las puertas de tu iglesia. Miramos tu adoración\"]\n",
    "\n",
    "tfidf_vec = tfidf_vec.fit(vocab)\n",
    "\n",
    "sentence1 = tfidf_vec.transform([u'puertas de tu iglesia'])\n",
    "sentence2 = tfidf_vec.transform(['adoración \\n'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %tfidf_vec.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: ['.', 'adoracion', 'iglesi', 'mir', 'puert']\n",
      "Sentence 1: [[0.         0.         0.70710678 0.         0.70710678]]\n",
      "Sentence 2: [[0. 1. 0. 0. 0.]]\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1dYYXoZrDzjX",
    "colab_type": "code",
    "outputId": "66d6f894-1c9f-47b2-ec1d-b5db161c8e0d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586123976844,
     "user_tz": 420,
     "elapsed": 418,
     "user": {
      "displayName": "Md Shadman Rafid",
      "photoUrl": "",
      "userId": "15946579363503205627"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    }
   },
   "source": [
    "tfidf_vec = tfidf_vec.fit(x_train.ravel())\n",
    "\n",
    "print('Vocabulary size: %s' %len(tfidf_vec.get_feature_names()))\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Vocabulary size: 648\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbehQHSVKeSs",
    "colab_type": "text"
   },
   "source": [
    "# **Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAXb-Y5AN3eD",
    "colab_type": "text"
   },
   "source": [
    "### **Models and F1-Score**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9aCU7H5OOBxr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Models: Multivariate Bernoulli and Multinomial naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Gj44RSsONWc",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Performance metric: F1-score\n",
    "\n",
    "# Custom scorer methods to account for positive-negative class labels\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# `pos_label` for positive class, since we have sad=1, happy=0\n",
    "\n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3CxXiJRPgLe",
    "colab_type": "text"
   },
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZrMU_bqIP14k",
    "colab_type": "code",
    "outputId": "ff7d8031-f9fa-4528-fc49-b41299918785",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071869848,
     "user_tz": 420,
     "elapsed": 7313,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    }
   },
   "source": [
    "!pip install scikit-learn"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XQ8QN6DlPcTs",
    "colab_type": "code",
    "outputId": "2c1de807-3cb4-46fb-8702-43c4592832f3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071878436,
     "user_tz": 420,
     "elapsed": 15890,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Grid Search with Count Vectorizer and Bernoulli Naive Bayes\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "pipeline_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', BernoulliNB())\n",
    "])\n",
    "\n",
    "parameters_1 = dict(\n",
    "    vect__binary=[True],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[snowball_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_1 = GridSearchCV(pipeline_1, \n",
    "                           parameters_1, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_1, depth=2)\n",
    "grid_search_1.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_1 = grid_search_1.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [True],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function snowball_tokenizer at 0x7f6cad3279d8>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best score: 0.274\n",
      "Best parameters set:\n",
      "\tvect__binary: True\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function snowball_tokenizer at 0x7f6cad3279d8>\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.5s finished\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7quPJyUlTIfs",
    "colab_type": "code",
    "outputId": "e71a4840-a27e-4be5-f86f-50d13d6279c3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071887318,
     "user_tz": 420,
     "elapsed": 24759,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Grid Search with Count Vectorizer and Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline_3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters_3 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[snowball_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_3 = GridSearchCV(pipeline_3, \n",
    "                           parameters_3, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_3, depth=2)\n",
    "grid_search_3.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_3 = grid_search_3.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_3.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [False],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function snowball_tokenizer at 0x7f6cad3279d8>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best score: 0.443\n",
      "Best parameters set:\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (2, 2)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function snowball_tokenizer at 0x7f6cad3279d8>\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.5s finished\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OQQU_Ko4WQwX",
    "colab_type": "code",
    "outputId": "6951d4fd-f852-43ba-dea9-ff50bc21f1fa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071896091,
     "user_tz": 420,
     "elapsed": 33521,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Grid Search with TfidfVectorizer and Bernoulli Naive Bayes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline_2 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', BernoulliNB())\n",
    "])\n",
    "\n",
    "parameters_2 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[snowball_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_2 = GridSearchCV(pipeline_2, \n",
    "                           parameters_2, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_2.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_2, depth=2)\n",
    "grid_search_2.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_2.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_2 = grid_search_2.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_2.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_2[param_name]))\n",
    "\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [False],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function snowball_tokenizer at 0x7f6cad3279d8>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best score: 0.274\n",
      "Best parameters set:\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function snowball_tokenizer at 0x7f6cad3279d8>\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.7s finished\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ot4SZ48nX-Yh",
    "colab_type": "code",
    "outputId": "2d38be61-840c-4b4c-d715-38851af350d3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071904654,
     "user_tz": 420,
     "elapsed": 42073,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Grid Search with TfidfVectorizer and Multinomial Naive Bayes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters_4 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[snowball_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_4 = GridSearchCV(pipeline_4, \n",
    "                           parameters_4, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_4, depth=2)\n",
    "grid_search_4.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_4 = grid_search_4.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_4.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [False],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function snowball_tokenizer at 0x7f6cad3279d8>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best score: 0.344\n",
      "Best parameters set:\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n",
      "\tvect__tokenizer: None\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.6s finished\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHszFoe8oZos",
    "colab_type": "text"
   },
   "source": [
    "# **VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BJtNdqJhonA1",
    "colab_type": "code",
    "outputId": "a387d71b-a678-426c-cafb-8da89e556c77",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071904854,
     "user_tz": 420,
     "elapsed": 42261,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    }
   },
   "source": [
    "final_clf = Pipeline([\n",
    "                ('vect', TfidfVectorizer(\n",
    "                                         binary=False,\n",
    "                                         stop_words=stop_words,\n",
    "                                         tokenizer=snowball_tokenizer,\n",
    "                                         ngram_range=(1,1),\n",
    "                                         )\n",
    "                ),\n",
    "                ('clf', MultinomialNB(alpha=1.0)),\n",
    "               ])\n",
    "final_clf.fit(x_train, y_train)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=['de', 'la', 'que', 'el', 'en', 'y',\n",
       "                                             'a', 'los', 'del', 'se', 'las',\n",
       "                                             'por', 'un', 'para', 'con', 'no',\n",
       "                                             'una', 'su', 'al', 'lo', 'como',\n",
       "                                             'más', 'pero', 'sus', 'le', 'ya',\n",
       "                                             'o', 'este', 'sí', 'porque', ...],\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function snowball_tokenizer at 0x7f6cad3279d8>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 59
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LZiMwPEXp4JG",
    "colab_type": "code",
    "outputId": "62d37472-9ef2-4e55-eac0-fd303968b3a0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071904855,
     "user_tz": 420,
     "elapsed": 42252,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "final_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(\n",
    "                                         binary=False,\n",
    "                                         stop_words=stop_words,\n",
    "                                         tokenizer=snowball_tokenizer,\n",
    "                                         ngram_range=(1,1),\n",
    "                                         )\n",
    "                ),\n",
    "                ('clf', MultinomialNB(alpha=1.0)),\n",
    "               ])\n",
    "final_clf.fit(x_train, y_train)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=['de', 'la', 'que', 'el', 'en', 'y',\n",
       "                                             'a', 'los', 'del', 'se', 'las',\n",
       "                                             'por', 'un', 'para', 'con', 'no',\n",
       "                                             'una', 'su', 'al', 'lo', 'como',\n",
       "                                             'más', 'pero', 'sus', 'le', 'ya',\n",
       "                                             'o', 'este', 'sí', 'porque', ...],\n",
       "                                 strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function snowball_tokenizer at 0x7f6cad3279d8>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S7Tydn6O_tq",
    "colab_type": "text"
   },
   "source": [
    "### TESTING ON TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LCsgKW6iqbAY",
    "colab_type": "code",
    "outputId": "8f2f2426-bf6c-4722-875d-6e7573bd08d9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071905299,
     "user_tz": 420,
     "elapsed": 42687,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    }
   },
   "source": [
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = metrics.confusion_matrix(y_train, final_clf.predict(x_train))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "mpl.rc(\"figure\", figsize=(4, 2))\n",
    "\n",
    "hm = sns.heatmap(cm, \n",
    "            cbar=False,\n",
    "            annot=True, \n",
    "            square=True,\n",
    "            fmt='d',\n",
    "            yticklabels=['happy','sad'],\n",
    "            xticklabels=['happy','sad'],\n",
    "            cmap='Blues'\n",
    "            )\n",
    "plt.title('Confusion matrix - Training dataset')\n",
    "plt.ylabel('actual class')\n",
    "plt.xlabel('predicted class')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/confmat_training.eps', dpi=300)\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAACICAYAAAAVvVzVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB4fKu970A5b",
    "colab_type": "text"
   },
   "source": [
    "### TESTING ON VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wzxoas2-tQ4r",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "DataFrame = pd.read_table('Spanish_validation_data.txt', delimiter=\"*\", names=('Link', 'Lyrics', 'Mood'))\n",
    "# print(df.head())\n",
    "\n",
    "X_valid = DataFrame['Lyrics'].values \n",
    "y_valid = DataFrame['Mood'].values\n",
    "\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jeueYzrO-dzh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# df = pd.read_csv('test_data-2.csv', delimiter='*', encoding=\"ISO-8859-1\")\n",
    "# print(df.head())\n",
    "# X_valid = df['Lyrics'].values \n",
    "# y_valid = df['Mood'].values\n",
    "\n",
    "# y_valid = le.transform(y_valid)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fXQWgWxBvipH",
    "colab_type": "code",
    "outputId": "0f87fef4-e888-4c70-e3e5-42d95d61c8be",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071922451,
     "user_tz": 420,
     "elapsed": 524,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    }
   },
   "source": [
    "cm = metrics.confusion_matrix(y_valid, final_clf.predict(X_valid))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "mpl.rc(\"figure\", figsize=(4, 2))\n",
    "\n",
    "hm = sns.heatmap(cm, \n",
    "            cbar=False,\n",
    "            annot=True, \n",
    "            square=True,\n",
    "            fmt='d',\n",
    "            yticklabels=['happy','sad'],\n",
    "            xticklabels=['happy','sad'],\n",
    "            cmap='Blues'\n",
    "            )\n",
    "plt.title('Confusion matrix - Validation dataset')\n",
    "plt.ylabel('actual class')\n",
    "plt.xlabel('predicted class')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/confmat_valid.eps', dpi=300)\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO8AAACICAYAAAAPgbxbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQIESlDcQKFCC8gYCBcr/A304jBWYQQT0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyDuoJZCPRDo",
    "colab_type": "text"
   },
   "source": [
    "## **METRICS**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ydb3qIOezbqu",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Custom scorer methods to account for positive-negative class labels\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# `pos_label` for positive class, since we have sad=1, happy=0\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n",
    "pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n",
    "rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)\n",
    "auc_scorer = metrics.make_scorer(metrics.roc_auc_score, greater_is_better=True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XnMCGsRWzc0_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "d = {'Data':['Training', 'Validation'],\n",
    "     'ACC (%)':[],\n",
    "     'PRE (%)':[],\n",
    "     'REC (%)':[],\n",
    "     'F1 (%)':[],\n",
    "     'ROC AUC (%)':[],\n",
    "}"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p5L3B5-Izi42",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n",
    "d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n",
    "d['REC (%)'].append(rec_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n",
    "d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n",
    "d['ROC AUC (%)'].append(auc_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n",
    "\n",
    "d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['ROC AUC (%)'].append(auc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z_2xYQsmzm_z",
    "colab_type": "code",
    "outputId": "7edf7901-76df-4b9d-94f5-e1c11e8267c7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586071944168,
     "user_tz": 420,
     "elapsed": 1246,
     "user": {
      "displayName": "Swati Mutyala",
      "photoUrl": "",
      "userId": "12420904876627817102"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    }
   },
   "source": [
    "DataFrame_perform = pd.DataFrame(d)\n",
    "DataFrame_perform = DataFrame_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)', 'ROC AUC (%)']]\n",
    "DataFrame_perform.index=(['Training', 'Validation'])\n",
    "DataFrame_perform = DataFrame_perform*100\n",
    "DataFrame_perform = np.round(DataFrame_perform, decimals=2)\n",
    "DataFrame_perform\n",
    "# DataFrame_perform.style.applymap(color_negative_red, subset=['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)', 'ROC AUC (%)'])\n",
    "# DataFrame_perform = np.round(DataFrame_perform, decimals=2)\n",
    "# DataFrame_perform"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC (%)</th>\n",
       "      <th>PRE (%)</th>\n",
       "      <th>REC (%)</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>ROC AUC (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>96.47</td>\n",
       "      <td>98.14</td>\n",
       "      <td>93.49</td>\n",
       "      <td>95.76</td>\n",
       "      <td>96.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>59.00</td>\n",
       "      <td>41.33</td>\n",
       "      <td>44.93</td>\n",
       "      <td>43.06</td>\n",
       "      <td>55.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ACC (%)  PRE (%)  REC (%)  F1 (%)  ROC AUC (%)\n",
       "Training      96.47    98.14    93.49   95.76        96.09\n",
       "Validation    59.00    41.33    44.93   43.06        55.67"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 69
    }
   ]
  }
 ]
}