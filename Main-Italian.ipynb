{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main-Italian.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMHnQGF4f+JC1+cjVG19zkT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sUd4N-U6kGGN","colab_type":"text"},"source":["# **Preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"s112tp0_kF1b","colab_type":"text"},"source":["Reading training DF"]},{"cell_type":"code","metadata":{"id":"zVlCbG_Kj_iN","colab_type":"code","outputId":"dda1b95a-cc79-425d-babf-2ab3bfea2eee","executionInfo":{"status":"ok","timestamp":1588217145323,"user_tz":420,"elapsed":447,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Training data of 400 songs\n","import pandas as pd\n","\n","df = pd.read_csv('/content/final_italian_training_data.csv')\n","df.columns = ['Lyrics', 'Mood']\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lyrics</th>\n","      <th>Mood</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Le stelle sono buchi nel cielo\\nDa cui filtra ...</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Amore mio .. Non sai quante volte ho sognato.....</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Piansero le deboli stelle\\nTre angeli dipinti ...</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Notte profonda senza luce in citt√†\\nLe strade...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Emi non andare via,\\nIo sono ancora qua,\\nCon ...</td>\n","      <td>Sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Lyrics   Mood\n","0  Le stelle sono buchi nel cielo\\nDa cui filtra ...  Happy\n","1  Amore mio .. Non sai quante volte ho sognato.....    Sad\n","2  Piansero le deboli stelle\\nTre angeli dipinti ...  Happy\n","3  Notte profonda senza luce in citt√†\\nLe strade...    Sad\n","4  Emi non andare via,\\nIo sono ancora qua,\\nCon ...    Sad"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"QTerOa0CkJ8q","colab_type":"text"},"source":["Encoding Mood Values"]},{"cell_type":"code","metadata":{"id":"5vXeTVAWkJ9w","colab_type":"code","outputId":"f04d1143-a91f-4fc4-96ec-01fba27511e4","executionInfo":{"status":"ok","timestamp":1588217165188,"user_tz":420,"elapsed":690,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.preprocessing import LabelEncoder\n","# import pickle\n","import numpy as np\n","\n","x_train = df['Lyrics'].values \n","y_train = df['Mood'].values\n","\n","print('before: %s ...' %y_train[:20])\n","\n","le = LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","\n","print('after: %s ...' %y_train[:20])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["before: ['Happy' 'Sad' 'Happy' 'Sad' 'Sad' 'Sad' 'Happy' 'Sad' 'Sad' 'Sad' 'Happy'\n"," 'Sad' 'Happy' 'Sad' 'Sad' 'Happy' 'Happy' 'Happy' 'Happy' 'Sad'] ...\n","after: [0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1] ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0UdIPZgtljcm","colab_type":"code","outputId":"d538e907-83c6-4dd9-e0a2-6b0e5c37ee08","executionInfo":{"status":"ok","timestamp":1587712880983,"user_tz":420,"elapsed":3959,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vmwOaAjNkO75","colab_type":"text"},"source":["Stemming (SnowballStemmer)"]},{"cell_type":"code","metadata":{"id":"aA55Ph3ulm3a","colab_type":"code","outputId":"f165b20a-bee6-4c70-cfee-ebef52d5807f","executionInfo":{"status":"ok","timestamp":1588217171006,"user_tz":420,"elapsed":440,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\" The following languages are supported: Danish, Dutch, English, Finnish, French, \n","German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish \n","and Swedish \"\"\"\n","import nltk\n","import re\n","from nltk import word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","\n","def snowball_tokenizer(text, stemmer = SnowballStemmer('italian')):\n","  lower_txt = text.lower()\n","  tokens = nltk.wordpunct_tokenize(lower_txt)\n","  stemmed_text = [stemmer.stem(i) for i in tokens]\n","  no_punct = [s for s in stemmed_text if re.match('^[a-zA-Z]+$', s) is not None]\n","  return stemmed_text\n","\n","\n","snowball_tokenizer(\"Sono qui sto guardando questa storia dritta ne.\")\n","\n","# stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['son', 'qui', 'sto', 'guard', 'quest', 'stor', 'dritt', 'ne', '.']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"v9EY8frArE8p","colab_type":"text"},"source":["Stopwords File"]},{"cell_type":"code","metadata":{"id":"0OZ0p0dvmxeA","colab_type":"code","outputId":"89cf5dab-6af3-4557-b948-ba92114f4be4","executionInfo":{"status":"ok","timestamp":1588217174791,"user_tz":420,"elapsed":437,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stp = stopwords.words('italian')\n","with open('./stopwords_italian.txt', 'w') as outfile:\n","   outfile.write('\\n'.join(stp))\n","\n","with open('./stopwords_italian.txt', 'r') as infile:\n","    stop_words = infile.read().splitlines()\n","print('stop words %s ...' %stop_words[:5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","stop words ['ad', 'al', 'allo', 'ai', 'agli'] ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6NHgf5jekUov","colab_type":"text"},"source":["# **VECTORIZING METHODS**"]},{"cell_type":"markdown","metadata":{"id":"BKD6QnZokUWD","colab_type":"text"},"source":["## Feature extraction: Word counts and Vectorizers"]},{"cell_type":"markdown","metadata":{"id":"KzRgqFLZkiSo","colab_type":"text"},"source":["Count Vectorizer"]},{"cell_type":"code","metadata":{"id":"SaBUFV2Yy07I","colab_type":"code","outputId":"68315d7e-30cd-4982-bae8-cc5e3dc01b7b","executionInfo":{"status":"ok","timestamp":1588217177459,"user_tz":420,"elapsed":684,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer,\n","            ngram_range=(1,1)\n","    )\n","\n","vocab = [\"Sono qui sto guardando questa storia dritta ne.\"]\n","\n","vec = vec.fit(vocab)\n","\n","sentence1 = vec.transform([u'guardando questa storia ne.'])\n","sentence2 = vec.transform(['dritta'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %vec.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['.', 'dritt', 'guard', 'quest', 'qui', 'son', 'stor']\n","Sentence 1: [[1 0 1 1 0 0 1]]\n","Sentence 2: [[0 1 0 0 0 0 0]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avro', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'piu', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"f079EV8_y3UY","colab_type":"code","outputId":"0de174f9-7eac-491c-d9b6-7cf6d8371c99","executionInfo":{"status":"ok","timestamp":1588217194469,"user_tz":420,"elapsed":14173,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["vec = vec.fit(x_train.ravel())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SVTff3PmkrV-","colab_type":"text"},"source":["TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"s8nFrWmBy5eg","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFtepnGgy73A","colab_type":"code","outputId":"67bd5687-5c5d-4e13-f24c-7aecb7005665","executionInfo":{"status":"ok","timestamp":1588217216096,"user_tz":420,"elapsed":14210,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["tfidf = tfidf.fit(x_train.ravel())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avro', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'piu', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FU_56MqbkwwA","colab_type":"text"},"source":["# **Model Selection**"]},{"cell_type":"markdown","metadata":{"id":"DcWraojfkwTD","colab_type":"text"},"source":["## **Models and F1-Score**"]},{"cell_type":"code","metadata":{"id":"36TNE0U5y_ct","colab_type":"code","colab":{}},"source":["#Models: Multivariate Bernoulli and Multinomial Naive Bayes\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.pipeline import Pipeline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngybuuQBzCvN","colab_type":"code","colab":{}},"source":["# Performance metric: F1-score\n","\n","# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3GGGjx-k4-a","colab_type":"text"},"source":["## **Grid** **Search**"]},{"cell_type":"code","metadata":{"id":"nfiQk9UPzE3y","colab_type":"code","outputId":"cb2242e5-7663-488a-bdaa-c22975259af7","executionInfo":{"status":"ok","timestamp":1587712965628,"user_tz":420,"elapsed":2983,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install scikit-learn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nxCMzdyCj8XB","colab_type":"code","outputId":"78a19a3e-a0a2-4d33-b2ac-df1ff35d2eed","executionInfo":{"status":"ok","timestamp":1587713832326,"user_tz":420,"elapsed":863413,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with Count Vectorizer and Bernoulli Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","\n","pipeline_1 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_1 = dict(\n","    vect__binary=[True],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_1 = GridSearchCV(pipeline_1, \n","                           parameters_1, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10                #Determines the cross-validation splitting strategy\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n","print(\"parameters:\")\n","pprint(parameters_1, depth=2)\n","grid_search_1.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_1 = grid_search_1.best_estimator_.get_params()\n","for param_name in sorted(parameters_1.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [True],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fc801c00510>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 14.2min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.168\n","Best parameters set:\n","\tvect__binary: True\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fc801c00510>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CG5hlFnOlFbb","colab_type":"code","outputId":"2d597d94-45da-40d2-d7ca-5f37418337e4","executionInfo":{"status":"ok","timestamp":1587252011502,"user_tz":420,"elapsed":206841,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with Count Vectorizer and Multinomial Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_3 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_3 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_3 = GridSearchCV(pipeline_3, \n","                           parameters_3, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n","print(\"parameters:\")\n","pprint(parameters_3, depth=2)\n","grid_search_3.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_3 = grid_search_3.best_estimator_.get_params()\n","for param_name in sorted(parameters_3.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fc4e24482f0>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  3.4min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.393\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fc4e24482f0>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UIwxXXS8mDIz","colab_type":"code","outputId":"ec914060-1b73-41f4-aa48-dc5d356000ac","executionInfo":{"status":"ok","timestamp":1587252297230,"user_tz":420,"elapsed":206991,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Bernoulli Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_2 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_2 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_2 = GridSearchCV(pipeline_2, \n","                           parameters_2, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_2.steps])\n","print(\"parameters:\")\n","pprint(parameters_2, depth=2)\n","grid_search_2.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_2.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_2 = grid_search_2.best_estimator_.get_params()\n","for param_name in sorted(parameters_2.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_2[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fc4e24482f0>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  3.4min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.265\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fc4e24482f0>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZVssX9OzmJXE","colab_type":"code","outputId":"971c1245-b9b1-440f-9a0d-ebbdee5c4c58","executionInfo":{"status":"ok","timestamp":1587252519454,"user_tz":420,"elapsed":204764,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Multinomial Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_4 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_4 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_4 = GridSearchCV(pipeline_4, \n","                           parameters_4, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n","print(\"parameters:\")\n","pprint(parameters_4, depth=2)\n","grid_search_4.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_4 = grid_search_4.best_estimator_.get_params()\n","for param_name in sorted(parameters_4.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fc4e24482f0>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  3.3min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.183\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (2, 2)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fc4e24482f0>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FV8pXbRqn5Wa","colab_type":"code","outputId":"9d8c62ec-1eda-4be4-c6fd-6b4578a99d7c","executionInfo":{"status":"ok","timestamp":1588217459267,"user_tz":420,"elapsed":13264,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["# Grid SEarch Best Score = 0.\n","final_clf = Pipeline([\n","                ('vect', CountVectorizer(\n","                                         binary=False,\n","                                         stop_words=stop_words,\n","                                         tokenizer=snowball_tokenizer,\n","                                         ngram_range=(1,1),\n","                                         )\n","                ),\n","                ('clf', MultinomialNB(alpha=1.0)),\n","               ])\n","final_clf.fit(x_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abbi', 'abbiam', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'com', 'contr', 'ebber', 'eran', 'erav', 'eravam', 'essend', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'hann', 'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'siam', 'sian', 'siat', 'siet', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'stare', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'tutt', 'vostr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=['ad', 'al', 'allo', 'ai', 'agli',\n","                                             'all', 'agl', 'alla', 'alle',\n","                                             'con', 'col', 'coi', 'da', 'dal',\n","                                             'dallo', 'dai', 'dagli', 'dall',\n","                                             'dagl', 'dalla', 'dalle', 'di',\n","                                             'del', 'dello', 'dei', 'degli',\n","                                             'dell', 'degl', 'della', 'delle', ...],\n","                                 strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=<function snowball_tokenizer at 0x7f4d279ce0d0>,\n","                                 vocabulary=None)),\n","                ('clf',\n","                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"3JfRBRutoBN0","colab_type":"code","outputId":"71731ad3-cf9a-46ef-9b16-327585169ae1","executionInfo":{"status":"ok","timestamp":1588217483295,"user_tz":420,"elapsed":13294,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import matplotlib as mpl\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","cm = metrics.confusion_matrix(y_train, final_clf.predict(x_train))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title('Italian Confusion matrix - Training dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('Italian_Training.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"4wB287JkoFS9","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","import pickle\n","import numpy as np\n","\n","df = pd.read_csv('/content/final_italian_testing_data.csv')\n","df.columns = ['Lyrics', 'Mood']\n","df.head()\n","\n","X_valid = df['Lyrics'].values \n","y_valid = df['Mood'].values\n","\n","\n","# le = LabelEncoder()\n","# le.fit(y_train)\n","y_valid = le.transform(y_valid)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgPHG0-aoL4P","colab_type":"code","outputId":"369188c8-7f4d-4c08-fddc-f55d1c6174b7","executionInfo":{"status":"ok","timestamp":1588217545187,"user_tz":420,"elapsed":2717,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["cm = metrics.confusion_matrix(y_valid, final_clf.predict(X_valid))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title('Italian Confusion matrix - Validation dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('Italian_Testing.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"phxIrOQzocAW","colab_type":"code","colab":{}},"source":["# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n","pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n","rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-_2WdvQodnd","colab_type":"code","colab":{}},"source":["d = {'Data':['Training', 'Validation'],\n","     'ACC (%)':[],\n","     'PRE (%)':[],\n","     'REC (%)':[],\n","     'F1 (%)':[],\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sy4KwX12onNh","colab_type":"code","colab":{}},"source":["d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","\n","d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jj9O4Brco42M","colab_type":"code","outputId":"5b89f8c8-ab12-4743-a354-3fd37e99a977","executionInfo":{"status":"ok","timestamp":1588217652524,"user_tz":420,"elapsed":565,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_perform = pd.DataFrame(d)\n","df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)']]\n","df_perform.index=(['Training', 'Validation'])\n","df_perform = df_perform*100\n","df_perform = np.round(df_perform, decimals=2)\n","df_perform"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ACC (%)</th>\n","      <th>PRE (%)</th>\n","      <th>REC (%)</th>\n","      <th>F1 (%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training</th>\n","      <td>90.55</td>\n","      <td>94.90</td>\n","      <td>75.60</td>\n","      <td>84.16</td>\n","    </tr>\n","    <tr>\n","      <th>Validation</th>\n","      <td>67.25</td>\n","      <td>75.56</td>\n","      <td>38.42</td>\n","      <td>50.94</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            ACC (%)  PRE (%)  REC (%)  F1 (%)\n","Training      90.55    94.90    75.60   84.16\n","Validation    67.25    75.56    38.42   50.94"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"_3xbzza8lSiv","colab_type":"code","colab":{}},"source":["df_perform.to_csv('italian_clf_performance.csv', index_label=False)"],"execution_count":0,"outputs":[]}]}