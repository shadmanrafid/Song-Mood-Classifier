{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main-Spanish.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GutwAaPGnz8w","colab_type":"text"},"source":["# **PREPROCESSING**"]},{"cell_type":"markdown","metadata":{"id":"TcKYRg3oElos","colab_type":"text"},"source":["Reading Training DF"]},{"cell_type":"code","metadata":{"id":"8MQzpPQRh7zh","colab_type":"code","outputId":"3e4dce84-3007-4c18-ec3b-0a20e1370ba1","executionInfo":{"status":"ok","timestamp":1588232329703,"user_tz":420,"elapsed":598,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Training data of 400 songs\n","import pandas as pd\n","\n","df = pd.read_csv('/content/spanish_training_data.csv')\n","df.columns = ['Lyrics', 'Mood']\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lyrics</th>\n","      <th>Mood</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Para mas luz marr√≥n\\nah√≠ ten√©s a un hombre ...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Es el creador redentor de todas las almas\\nrio...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Algo se paso aqui por esta alma\\nno recuerdo s...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Claro es, verte tan herida.\\nTe sorprende verm...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Con tu rouge pint√°s todo, te ver√°s m√°s que ...</td>\n","      <td>Sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Lyrics Mood\n","0  Para mas luz marr√≥n\\nah√≠ ten√©s a un hombre ...  Sad\n","1  Es el creador redentor de todas las almas\\nrio...  Sad\n","2  Algo se paso aqui por esta alma\\nno recuerdo s...  Sad\n","3  Claro es, verte tan herida.\\nTe sorprende verm...  Sad\n","4  Con tu rouge pint√°s todo, te ver√°s m√°s que ...  Sad"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"tw6GiDS6FahD","colab_type":"text"},"source":["Encoding Mood Values"]},{"cell_type":"code","metadata":{"id":"XN6iyjM_4_z2","colab_type":"code","outputId":"ea6684de-4b4e-4757-bdb1-079eefb44759","executionInfo":{"status":"ok","timestamp":1588232343068,"user_tz":420,"elapsed":361,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.preprocessing import LabelEncoder\n","# import pickle\n","import numpy as np\n","\n","x_train = df['Lyrics'].values \n","y_train = df['Mood'].values\n","\n","print('before: %s ...' %y_train[:5])\n","\n","le = LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","\n","print('after: %s ...' %y_train[:5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["before: ['Sad' 'Sad' 'Sad' 'Sad' 'Sad'] ...\n","after: [1 1 1 1 1] ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aH8dZm14RGpa","colab_type":"code","outputId":"8a2caca7-9e87-4ded-8415-d07a48c3f60f","executionInfo":{"status":"ok","timestamp":1588232348983,"user_tz":420,"elapsed":3060,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IGv6AmnGFxuX","colab_type":"text"},"source":["Stemming (SnowballStemmer)"]},{"cell_type":"code","metadata":{"id":"KJSkoDB36s8L","colab_type":"code","outputId":"f8a4776e-9646-4682-c407-0b5582a0d416","executionInfo":{"status":"ok","timestamp":1588232352490,"user_tz":420,"elapsed":357,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\" The following languages are supported: Danish, Dutch, English, Finnish, French, \n","German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish \n","and Swedish \"\"\"\n","import nltk\n","import re\n","from nltk import word_tokenize\n","from nltk.stem import SnowballStemmer\n","# stemmer = SnowballStemmer('spanish')\n","\n","\n","def snowball_tokenizer(text, stemmer = SnowballStemmer('spanish')):\n","  lower_txt = text.lower()\n","  tokens = nltk.wordpunct_tokenize(lower_txt)\n","  stemmed_text = [stemmer.stem(i) for i in tokens]\n","  no_punct = [s for s in stemmed_text if re.match('^[a-zA-Z]+$', s) is not None]\n","  return stemmed_text\n","\n","\n","snowball_tokenizer(\"A través de un libro\")\n","\n","# stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a', 'traves', 'de', 'un', 'libr']"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"OHdBzUIHGBMz","colab_type":"text"},"source":["Stopwords File"]},{"cell_type":"code","metadata":{"id":"FOXPOKQs8uQw","colab_type":"code","outputId":"44403166-929d-44fc-a001-d796abd583f8","executionInfo":{"status":"ok","timestamp":1588232374633,"user_tz":420,"elapsed":528,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stp = stopwords.words('spanish')\n","with open('./stopwords_spanish.txt', 'w') as outfile:\n","   outfile.write('\\n'.join(stp))\n","\n","with open('./stopwords_spanish.txt', 'r') as infile:\n","    stop_words = infile.read().splitlines()\n","print('stop words %s ...' %stop_words[:5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","stop words ['de', 'la', 'que', 'el', 'en'] ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TgWgBsMNOnZY","colab_type":"text"},"source":["### **VECTORIZING** **METHODS**"]},{"cell_type":"markdown","metadata":{"id":"gqwpweZR2OHG","colab_type":"text"},"source":["\n","## Feature extraction: Word counts and Vectorizers"]},{"cell_type":"markdown","metadata":{"id":"hduH-1LrOLP9","colab_type":"text"},"source":["Count Vectorizer "]},{"cell_type":"code","metadata":{"id":"w0KUG-oI-TZw","colab_type":"code","outputId":"06eba6a7-fdac-4306-f08a-f4e1243c868f","executionInfo":{"status":"ok","timestamp":1588232378801,"user_tz":420,"elapsed":344,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer,\n","            ngram_range=(1,1)\n","    )\n","\n","vocab = [\"En las puertas de tu iglesia. Miramos tu adoración\"]\n","\n","vec = vec.fit(vocab)\n","\n","sentence1 = vec.transform([u'puertas de tu iglesia'])\n","sentence2 = vec.transform(['adoración \\n'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %vec.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['.', 'adoracion', 'iglesi', 'mir', 'puert']\n","Sentence 1: [[0 0 1 0 1]]\n","Sentence 2: [[0 1 0 0 0]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KWn3_wXa_6bo","colab_type":"code","outputId":"1fd4beae-549c-4750-db48-b910619e497f","executionInfo":{"status":"ok","timestamp":1588232392246,"user_tz":420,"elapsed":10590,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["vec = vec.fit(x_train.ravel())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7m8MgTCXOc4N","colab_type":"text"},"source":["Count Vectorizer (For 2-gram)"]},{"cell_type":"code","metadata":{"id":"FCdBowyRCUam","colab_type":"code","outputId":"1a79f02c-9bde-45cf-d375-1f3d4f4b6486","executionInfo":{"status":"ok","timestamp":1588232397402,"user_tz":420,"elapsed":465,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["vec = CountVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer,\n","            ngram_range=(2,2)\n","    )\n","\n","vocab = [\"En las puertas de tu iglesia. Miramos tu adoración\"]\n","\n","vec = vec.fit(vocab)\n","\n","sentence1 = vec.transform([u'puertas de tu iglesia'])\n","sentence2 = vec.transform(['adoración \\n'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %vec.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['. mir', 'iglesi .', 'mir adoracion', 'puert iglesi']\n","Sentence 1: [[0 0 0 1]]\n","Sentence 2: [[0 0 0 0]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9TrB6W5TOkz3","colab_type":"text"},"source":["TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"UDt3JmRVCk4K","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(\n","            encoding='utf-8',\n","            decode_error='replace',\n","            strip_accents='unicode',\n","            analyzer='word',\n","            binary=False,\n","            stop_words=stop_words,\n","            tokenizer=snowball_tokenizer\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E142GHG1CmFM","colab_type":"code","outputId":"b0256892-caec-4877-da1a-85f094c92c14","executionInfo":{"status":"ok","timestamp":1588232410290,"user_tz":420,"elapsed":383,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["vocab = [\"En las puertas de tu iglesia. Miramos tu adoración\"]\n","\n","tfidf = tfidf.fit(vocab)\n","\n","sentence1 = tfidf.transform([u'puertas de tu iglesia'])\n","sentence2 = tfidf.transform(['adoración \\n'])\n","\n","\n","print('TEST:')\n","print('Vocabulary: %s' %tfidf.get_feature_names())\n","print('Sentence 1: %s' %sentence1.toarray())\n","print('Sentence 2: %s' %sentence2.toarray())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TEST:\n","Vocabulary: ['.', 'adoracion', 'iglesi', 'mir', 'puert']\n","Sentence 1: [[0.         0.         0.70710678 0.         0.70710678]]\n","Sentence 2: [[0. 1. 0. 0. 0.]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1dYYXoZrDzjX","colab_type":"code","outputId":"46393da2-7682-4156-f0a1-5d4958c706ce","executionInfo":{"status":"ok","timestamp":1588232426221,"user_tz":420,"elapsed":10875,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["tfidf = tfidf.fit(x_train.ravel())\n","\n","print('Vocabulary size: %s' %len(tfidf.get_feature_names()))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"},{"output_type":"stream","text":["Vocabulary size: 13917\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gbehQHSVKeSs","colab_type":"text"},"source":["# **Model Selection**"]},{"cell_type":"markdown","metadata":{"id":"TAXb-Y5AN3eD","colab_type":"text"},"source":["### **Models and F1-Score**"]},{"cell_type":"code","metadata":{"id":"9aCU7H5OOBxr","colab_type":"code","colab":{}},"source":["#Models: Multivariate Bernoulli and Multinomial Naive Bayes\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.pipeline import Pipeline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Gj44RSsONWc","colab_type":"code","colab":{}},"source":["# Performance metric: F1-score\n","\n","# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3CxXiJRPgLe","colab_type":"text"},"source":["### **Grid Search**"]},{"cell_type":"code","metadata":{"id":"ZrMU_bqIP14k","colab_type":"code","outputId":"b22fd2e1-6ca6-4846-b1b7-de0e35fe7d81","executionInfo":{"status":"ok","timestamp":1588232447622,"user_tz":420,"elapsed":3202,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install scikit-learn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XQ8QN6DlPcTs","colab_type":"code","outputId":"335c820e-c965-40e3-f080-3a9929610950","executionInfo":{"status":"error","timestamp":1588218372652,"user_tz":420,"elapsed":4006,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":582}},"source":["# Grid Search with Count Vectorizer and Bernoulli Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","\n","pipeline_1 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_1 = dict(\n","    vect__binary=[True],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_1 = GridSearchCV(pipeline_1, \n","                           parameters_1, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10                #Determines the cross-validation splitting strategy\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n","print(\"parameters:\")\n","pprint(parameters_1, depth=2)\n","grid_search_1.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_1 = grid_search_1.best_estimator_.get_params()\n","for param_name in sorted(parameters_1.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [True],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fdbea082510>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b502ccde7979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mgrid_search_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_search_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-707080e7d387>\u001b[0m in \u001b[0;36msnowball_tokenizer\u001b[0;34m(text, stemmer)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlower_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mstemmed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mno_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmed_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^[a-zA-Z]+$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstemmed_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-707080e7d387>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlower_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mstemmed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mno_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmed_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^[a-zA-Z]+$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstemmed_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   4039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4040\u001b[0m         \u001b[0;31m# STEP 2b: Other verb suffixes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4041\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step2b_suffixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4042\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4043\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"7quPJyUlTIfs","colab_type":"code","outputId":"211b42ae-a0c9-41c8-828a-429cdfe9809e","executionInfo":{"status":"ok","timestamp":1587456254971,"user_tz":420,"elapsed":11954,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with Count Vectorizer and Multinomial Naive Bayes\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_3 = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_3 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_3 = GridSearchCV(pipeline_3, \n","                           parameters_3, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n","print(\"parameters:\")\n","pprint(parameters_3, depth=2)\n","grid_search_3.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_3 = grid_search_3.best_estimator_.get_params()\n","for param_name in sorted(parameters_3.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fe64f598400>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.443\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (2, 2)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fe64f598400>\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.5s finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OQQU_Ko4WQwX","colab_type":"code","outputId":"0a749859-5010-4a07-80fd-2588fd9b7476","executionInfo":{"status":"ok","timestamp":1587456270248,"user_tz":420,"elapsed":11956,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Bernoulli Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_2 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', BernoulliNB())\n","])\n","\n","parameters_2 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_2 = GridSearchCV(pipeline_2, \n","                           parameters_2, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_2.steps])\n","print(\"parameters:\")\n","pprint(parameters_2, depth=2)\n","grid_search_2.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_2.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_2 = grid_search_2.best_estimator_.get_params()\n","for param_name in sorted(parameters_2.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_2[param_name]))\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fe64f598400>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.274\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: None\n","\tvect__tokenizer: <function snowball_tokenizer at 0x7fe64f598400>\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.7s finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Ot4SZ48nX-Yh","colab_type":"code","outputId":"782b8a37-cb99-4de7-d28c-184ac09de6d8","executionInfo":{"status":"ok","timestamp":1587456283044,"user_tz":420,"elapsed":9620,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Grid Search with TfidfVectorizer and Multinomial Naive Bayes\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_4 = Pipeline([\n","    ('vect', TfidfVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","parameters_4 = dict(\n","    vect__binary=[False],\n","    vect__stop_words=[stop_words, None],\n","    vect__tokenizer=[snowball_tokenizer, None],\n","    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",")\n","\n","grid_search_4 = GridSearchCV(pipeline_4, \n","                           parameters_4, \n","                           n_jobs=1, \n","                           verbose=1,\n","                           scoring=f1_scorer,\n","                           cv=10\n","                )\n","\n","\n","print(\"Performing grid search...\")\n","print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n","print(\"parameters:\")\n","pprint(parameters_4, depth=2)\n","grid_search_4.fit(x_train, y_train)\n","print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters_4 = grid_search_4.best_estimator_.get_params()\n","for param_name in sorted(parameters_4.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performing grid search...\n","pipeline: ['vect', 'clf']\n","parameters:\n","{'vect__binary': [False],\n"," 'vect__ngram_range': [(...), (...), (...)],\n"," 'vect__stop_words': [[...], None],\n"," 'vect__tokenizer': [<function snowball_tokenizer at 0x7fe64f598400>, None]}\n","Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Best score: 0.344\n","Best parameters set:\n","\tvect__binary: False\n","\tvect__ngram_range: (1, 1)\n","\tvect__stop_words: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n","\tvect__tokenizer: None\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    8.7s finished\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"DHszFoe8oZos","colab_type":"text"},"source":["# **VALIDATION**"]},{"cell_type":"code","metadata":{"id":"BJtNdqJhonA1","colab_type":"code","outputId":"35332523-8376-442c-de0a-cd89095b8488","executionInfo":{"status":"ok","timestamp":1587900651703,"user_tz":420,"elapsed":10971,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["# Grid Search Best Score = 0.344\n","final_clf = Pipeline([\n","                ('vect', TfidfVectorizer(\n","                                         binary=False,\n","                                         stop_words=stop_words,\n","                                         tokenizer=snowball_tokenizer,\n","                                         ngram_range=(1,1),\n","                                         )\n","                ),\n","                ('clf', MultinomialNB(alpha=1.0)),\n","               ])\n","final_clf.fit(x_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 TfidfVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.float64'>,\n","                                 encoding='utf-8', input='content',\n","                                 lowercase=True, max_df=1.0, max_features=None,\n","                                 min_df=1, ngram_range=(1, 1), norm='l2',\n","                                 preprocessor=None, smooth_idf=True,\n","                                 stop_words=['de', 'la', 'que', 'el', 'en', 'y',\n","                                             'a', 'los', 'del', 'se', 'las',\n","                                             'por', 'un', 'para', 'con', 'no',\n","                                             'una', 'su', 'al', 'lo', 'como',\n","                                             'más', 'pero', 'sus', 'le', 'ya',\n","                                             'o', 'este', 'sí', 'porque', ...],\n","                                 strip_accents=None, sublinear_tf=False,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=<function snowball_tokenizer at 0x7fc981393488>,\n","                                 use_idf=True, vocabulary=None)),\n","                ('clf',\n","                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"id":"LZiMwPEXp4JG","colab_type":"code","outputId":"d8cdf4c7-116a-4c4a-d77d-c0974c771615","executionInfo":{"status":"ok","timestamp":1588232470951,"user_tz":420,"elapsed":10739,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["# Grid SEarch Best Score = 0.443\n","final_clf = Pipeline([\n","                ('vect', CountVectorizer(\n","                                         binary=False,\n","                                         stop_words=stop_words,\n","                                         tokenizer=snowball_tokenizer,\n","                                         ngram_range=(1,1),\n","                                         )\n","                ),\n","                ('clf', MultinomialNB(alpha=1.0)),\n","               ])\n","final_clf.fit(x_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=['de', 'la', 'que', 'el', 'en', 'y',\n","                                             'a', 'los', 'del', 'se', 'las',\n","                                             'por', 'un', 'para', 'con', 'no',\n","                                             'una', 'su', 'al', 'lo', 'como',\n","                                             'más', 'pero', 'sus', 'le', 'ya',\n","                                             'o', 'este', 'sí', 'porque', ...],\n","                                 strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=<function snowball_tokenizer at 0x7fdbddb679d8>,\n","                                 vocabulary=None)),\n","                ('clf',\n","                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"7S7Tydn6O_tq","colab_type":"text"},"source":["### TESTING ON TRAINING DATA"]},{"cell_type":"code","metadata":{"id":"LCsgKW6iqbAY","colab_type":"code","outputId":"5643f920-1850-44f5-ddba-cd5d39c4d144","executionInfo":{"status":"ok","timestamp":1588232508622,"user_tz":420,"elapsed":10662,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import matplotlib as mpl\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","cm = metrics.confusion_matrix(y_train, final_clf.predict(x_train))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title(' Spanish Confusion Matrix - Training dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('Spanish_Training.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"NB4fKu970A5b","colab_type":"text"},"source":["### TESTING ON VALIDATION DATA"]},{"cell_type":"code","metadata":{"id":"Wzxoas2-tQ4r","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","import pickle\n","import numpy as np\n","\n","df = pd.read_csv('/content/spanish_testing_data_2.csv')\n","df.columns = ['Lyrics', 'Mood']\n","\n","\n","X_valid = df['Lyrics'].values \n","y_valid = df['Mood'].values\n","\n","\n","y_valid = le.transform(y_valid)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vptBI5BpQkIF","colab_type":"code","outputId":"eea1a4fa-8325-41af-a89a-70ea668ddba5","executionInfo":{"status":"ok","timestamp":1588233033328,"user_tz":420,"elapsed":402,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lyrics</th>\n","      <th>Mood</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Para mas luz marr√≥n\\nah√≠ ten√©s a un hombre ...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Es el creador redentor de todas las almas\\nrio...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Algo se paso aqui por esta alma\\nno recuerdo s...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Claro es, verte tan herida.\\nTe sorprende verm...</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Con tu rouge pint√°s todo, te ver√°s m√°s que ...</td>\n","      <td>Sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Lyrics Mood\n","0  Para mas luz marr√≥n\\nah√≠ ten√©s a un hombre ...  Sad\n","1  Es el creador redentor de todas las almas\\nrio...  Sad\n","2  Algo se paso aqui por esta alma\\nno recuerdo s...  Sad\n","3  Claro es, verte tan herida.\\nTe sorprende verm...  Sad\n","4  Con tu rouge pint√°s todo, te ver√°s m√°s que ...  Sad"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"w2q97sjy5FPs","colab_type":"text"},"source":["# **ACTUAL CLASSIFICATION**"]},{"cell_type":"code","metadata":{"id":"fXQWgWxBvipH","colab_type":"code","outputId":"cdf56f98-b50a-4d77-bf87-14aef85a6943","executionInfo":{"status":"ok","timestamp":1588233039708,"user_tz":420,"elapsed":898,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["cm = metrics.confusion_matrix(y_valid, final_clf.predict(X_valid))\n","\n","np.set_printoptions(suppress=True)\n","mpl.rc(\"figure\", figsize=(4, 2))\n","\n","hm = sns.heatmap(cm, \n","            cbar=False,\n","            annot=True, \n","            square=True,\n","            fmt='d',\n","            yticklabels=['happy','sad'],\n","            xticklabels=['happy','sad'],\n","            cmap='Blues'\n","            )\n","plt.title('Spanish Confusion Matrix - Validation dataset')\n","plt.ylabel('actual class')\n","plt.xlabel('predicted class')\n","plt.tight_layout()\n","plt.savefig('Spanish_Testing.eps', dpi=300)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"uyDuoJZCPRDo","colab_type":"text"},"source":["## **METRICS**"]},{"cell_type":"code","metadata":{"id":"ydb3qIOezbqu","colab_type":"code","colab":{}},"source":["# Custom scorer methods to account for positive-negative class labels\n","\n","from sklearn import metrics\n","\n","# `pos_label` for positive class, since we have sad=1, happy=0\n","\n","acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n","pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n","rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n","f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnMCGsRWzc0_","colab_type":"code","colab":{}},"source":["d = {'Data':['Training', 'Validation'],\n","     'ACC (%)':[],\n","     'PRE (%)':[],\n","     'REC (%)':[],\n","     'F1 (%)':[],\n","     \n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5L3B5-Izi42","colab_type":"code","colab":{}},"source":["d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=x_train, y_true=y_train))\n","\n","d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n","d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_2xYQsmzm_z","colab_type":"code","outputId":"116facfd-d25b-4c68-d60c-f946f758582b","executionInfo":{"status":"ok","timestamp":1588233096697,"user_tz":420,"elapsed":451,"user":{"displayName":"Swati Mutyala","photoUrl":"","userId":"12420904876627817102"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_perform = pd.DataFrame(d)\n","df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)']]\n","df_perform.index=(['Training', 'Validation'])\n","df_perform = df_perform*100\n","df_perform = np.round(df_perform, decimals=2)\n","df_perform\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ACC (%)</th>\n","      <th>PRE (%)</th>\n","      <th>REC (%)</th>\n","      <th>F1 (%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training</th>\n","      <td>86.55</td>\n","      <td>93.40</td>\n","      <td>69.31</td>\n","      <td>79.57</td>\n","    </tr>\n","    <tr>\n","      <th>Validation</th>\n","      <td>71.75</td>\n","      <td>81.08</td>\n","      <td>37.74</td>\n","      <td>51.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            ACC (%)  PRE (%)  REC (%)  F1 (%)\n","Training      86.55    93.40    69.31   79.57\n","Validation    71.75    81.08    37.74   51.50"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"_Doy3xcN0CYR","colab_type":"code","colab":{}},"source":["df_perform.to_csv('spanish_clf_performance.csv', index_label=False)"],"execution_count":0,"outputs":[]}]}